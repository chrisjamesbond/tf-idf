{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Words that occur nearby frequently are more important than words that only appear once or twice. Yet words that are too frequent — ubiquitous, like \"the\" or \"good\" — are unimportant. How can we balance these two conflicting constraints?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "from functools import reduce\n",
    "from collections import defaultdict\n",
    "\n",
    "# directory where documents are stored\n",
    "documents_directory = os.path.expanduser('documents')\n",
    "documents = os.listdir(documents_directory)\n",
    "documents.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gets a set of unique terms from a specific document\n",
    "def get_all_terms_from_document(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        document = file.read()\n",
    "    # split document into terms and convert to a set of unique terms\n",
    "    terms = document.split()\n",
    "    return terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gets all the unique terms for a set of documents in a directory\n",
    "def get_all_unique_terms(directory):\n",
    "    unique_terms = []\n",
    "\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            unique_terms.extend(get_all_terms_from_document(file_path))\n",
    "\n",
    "    unique_terms = set(unique_terms)\n",
    "\n",
    "    return unique_terms\n",
    "\n",
    "unique_terms = list(get_all_unique_terms(documents_directory))\n",
    "unique_terms.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents 5\n",
      "Number of terms 32575\n"
     ]
    }
   ],
   "source": [
    "num_documents = len([name for name in os.listdir(documents_directory) if os.path.isfile(os.path.join(documents_directory, name))])\n",
    "num_terms = len(unique_terms)\n",
    "\n",
    "print(f'Number of documents {num_documents}')\n",
    "print(f'Number of terms {num_terms}')\n",
    "\n",
    "# init the term_frequency matrix\n",
    "term_frequency = np.zeros((num_terms, num_documents), dtype=float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to get the count of terms for each document and store them in a matrix of size t x d\n",
    "$$\n",
    "\\text{term\\_frequency}_{term,document} = \\text{count}(term, document)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a couple helper functions to get our indicies from labels\n",
    "term_to_index = {term: idx for idx, term in enumerate(unique_terms)}\n",
    "document_to_index = {document: idx for idx, document in enumerate(documents)}\n",
    "\n",
    "# get the counts of terms in each document\n",
    "def compute_term_frequency_matrix(directory, documents, tf):\n",
    "    for document in documents:\n",
    "        file_path = os.path.join(directory, document)\n",
    "        all_terms_in_document = get_all_terms_from_document(file_path)\n",
    "\n",
    "        word_frequencies = reduce(lambda freq, word: freq.update({word: freq.get(word, 0) + 1}) or freq, all_terms_in_document, defaultdict(int))\n",
    "        word_frequencies = dict(word_frequencies)\n",
    "\n",
    "        for term in word_frequencies.keys():\n",
    "            term_frequency[term_to_index[term], document_to_index[document]] = word_frequencies[term]\n",
    "    \n",
    "compute_term_frequency_matrix(documents_directory, documents, term_frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term frequency for ebook:\n",
      "1342.txt: 13\n",
      "145.txt: 13\n",
      "1513.txt: 13\n",
      "2641.txt: 6\n",
      "2701.txt: 13\n"
     ]
    }
   ],
   "source": [
    "# we can look up individual terms \n",
    "\n",
    "term = 'ebook'\n",
    "\n",
    "def term_lookup(term, term_frequency):\n",
    "    term_index = term_to_index[term]\n",
    "    print(f'Term frequency for {term}:')\n",
    "    for document in documents:\n",
    "        document_index = document_to_index[document]\n",
    "        print(f'{document}: {int(term_frequency[term_index, document_index])}')\n",
    "    \n",
    "\n",
    "term_lookup(term, term_frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's squash the raw frequency using log10. A word appearing 100 times in a document doesn’t make that word 100 times more likely to be relevant to the meaning of the document. We also need to do something special with counts of 0, since we can’t take the log of 0.\n",
    "\n",
    "$$\n",
    "\\text{term\\_frequency}_{term,document} =\n",
    "\\begin{cases}\n",
    "1 + \\log_{10}(\\text{count}(term,document)) & \\text{if } \\text{count}(term,document) > 0 \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term frequency for ebook:\n",
      "1342.txt: 13\n",
      "145.txt: 13\n",
      "1513.txt: 13\n",
      "2641.txt: 6\n",
      "2701.txt: 13\n",
      "Term frequency for ebook:\n",
      "1342.txt: 27\n",
      "145.txt: 27\n",
      "1513.txt: 27\n",
      "2641.txt: 13\n",
      "2701.txt: 27\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "result = term_frequency*2 + 1\n",
    "term_lookup(term, term_frequency)\n",
    "term_lookup(term, result)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "squash = lambda x: 0 if x == 0 else 1 + math.log(14,5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
